{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for optimal imagenet images that induce the largest projection values onto the singular vectors/product of U and V inside the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as c\n",
    "from load_model import load_model\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model_names = [\n",
    "\"google/vit-base-patch16-224\",\n",
    "\"google/vit-base-patch32-384\",\n",
    "\"google/vit-large-patch16-224\",\n",
    "\"google/vit-large-patch32-384\",\n",
    "\"google/vit-huge-patch14-224-in21k\",\n",
    "\"facebook/dino-vitb16\",\n",
    "\"facebook/dino-vits16\",\n",
    "\"openai/clip-vit-base-patch16\",\n",
    "\"openai/clip-vit-base-patch32\",\n",
    "\"openai/clip-vit-large-patch14\",\n",
    "\"facebook/deit-base-distilled-patch16-224\",\n",
    "\"facebook/deit-small-distilled-patch16-224\",\n",
    "\"facebook/deit-tiny-distilled-patch16-224\"\n",
    "]\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    (model, processor) = load_model(model_name) \n",
    "        \n",
    "    c.imagenet_val_path = \"/opt/scratch/group/odelia/vit_2023/ImageNet1k/ILSVRC/Data/CLS-LOC/val/\"\n",
    "    im_list = os.listdir(c.imagenet_val_path)\n",
    "\n",
    "    if 'deit' in model_name: # deit has two extra tokens before image patch tokens.\n",
    "        start_index = 2\n",
    "    else:\n",
    "        start_index = 1\n",
    "\n",
    "    for pn in [\"p\", \"n\"]: # direction of singular vectors are arbitary. so look for both direction.\n",
    "\n",
    "        U_total = np.load(os.path.join(c.data_path, \"UVS\", f\"{model_name}_U_total.npy\"))\n",
    "        Vt_total = np.load(os.path.join(c.data_path, \"UVS\", f\"{model_name}_Vt_total.npy\"))\n",
    "\n",
    "        if pn == \"n\":\n",
    "            U_total = -U_total\n",
    "            Vt_total = -Vt_total\n",
    "\n",
    "        U_total = torch.from_numpy(U_total).float().to(c.device) # left singular vector\n",
    "        Vt_total = torch.from_numpy(Vt_total).float().to(c.device) # right singular vector\n",
    "\n",
    "        total_U = [] # the activation\n",
    "        total_V = []\n",
    "        total_U_max = [] # the activation\n",
    "        total_V_max = []\n",
    "        total_U_max5 = [] # the activation\n",
    "        total_V_max5 = []\n",
    "        total_product = [] # the activation\n",
    "\n",
    "        for im_file in tqdm(im_list):\n",
    "            im = plt.imread(c.imagenet_val_path+im_file)\n",
    "            if len(im.shape) < 3: # gray images\n",
    "                im = np.repeat(im[:, :, np.newaxis], 3, axis=2)\n",
    "            input = processor(images=im, return_tensors=\"pt\")\n",
    "            input = input[\"pixel_values\"].float().to(c.device)\n",
    "            output = model(input, output_hidden_states=True, output_attentions=False)\n",
    "\n",
    "            layer_U = []\n",
    "            layer_V = []\n",
    "            layer_U_max = []\n",
    "            layer_V_max = []\n",
    "            layer_U_max5 = []\n",
    "            layer_V_max5 = []\n",
    "            layer_product = []\n",
    "            for layer in range(Vt_total.shape[0]):\n",
    "                hidden_states = output['hidden_states'][layer] # 1 batch, 14*14+1 token, 12 head * 64 embeding\n",
    "                head_U = []\n",
    "                head_V = []\n",
    "                head_U_max = []\n",
    "                head_V_max = []\n",
    "                head_U_max5 = []\n",
    "                head_V_max5 = []\n",
    "                head_product = []\n",
    "                for head in range(Vt_total.shape[1]):\n",
    "                    head_U.append(torch.mean((hidden_states[:,:,:] @ U_total[layer,head,:,:])[:,start_index:,:], dim=1)[0].detach().to('cpu').numpy())\n",
    "                    head_V.append(torch.mean((hidden_states[:,:,:] @ (Vt_total[layer,head,:,:].T))[:,start_index:,:], dim=1)[0].detach().to('cpu').numpy())\n",
    "                    head_U_max.append(torch.max((hidden_states[:,:,:] @ U_total[layer,head,:,:])[:,start_index:,:], dim=1)[0][0].detach().to('cpu').numpy())\n",
    "                    head_V_max.append(torch.max((hidden_states[:,:,:] @ (Vt_total[layer,head,:,:].T))[:,start_index:,:], dim=1)[0][0].detach().to('cpu').numpy())\n",
    "                    head_U_max5.append(torch.mean(torch.topk((hidden_states[:,:,:] @ U_total[layer,head,:,:])[:,start_index:,:], 5, dim=1)[0], dim=1)[0].detach().to('cpu').numpy())\n",
    "                    head_V_max5.append(torch.mean(torch.topk((hidden_states[:,:,:] @ Vt_total[layer,head,:,:].T)[:,start_index:,:], 5, dim=1)[0], dim=1)[0].detach().to('cpu').numpy())\n",
    "                    head_product.append(torch.max((hidden_states[:,:,:] @ U_total[layer,head,:,:])[:,start_index:,:], dim=1)[0][0].detach().to('cpu').numpy() * \n",
    "                                        torch.max((hidden_states[:,:,:] @ (Vt_total[layer,head,:,:].T))[:,start_index:,:], dim=1)[0][0].detach().to('cpu').numpy())\n",
    "                layer_U.append(head_U)\n",
    "                layer_V.append(head_V)\n",
    "                layer_U_max.append(head_U_max)\n",
    "                layer_V_max.append(head_V_max)\n",
    "                layer_U_max5.append(head_U_max5)\n",
    "                layer_V_max5.append(head_V_max5)\n",
    "                layer_product.append(head_product)\n",
    "            total_U.append(layer_U)\n",
    "            total_V.append(layer_V)\n",
    "            total_U_max.append(layer_U_max)\n",
    "            total_V_max.append(layer_V_max)\n",
    "            total_U_max5.append(layer_U_max5)\n",
    "            total_V_max5.append(layer_V_max5)\n",
    "            total_product.append(layer_product)\n",
    "\n",
    "        file_path = os.path.join(c.data_path, \"optimal_natural_image/data\", f\"{model_name}_mean_{pn}.npy\")\n",
    "        if not os.path.exists(os.path.dirname(file_path)):\n",
    "            os.makedirs(os.path.dirname(file_path))\n",
    "        np.save(file_path, np.array([total_U, total_V]))\n",
    "        np.save(os.path.join(c.data_path, \"optimal_natural_image/data\", f\"{model_name}_max_{pn}.npy\"), np.array([total_U_max, total_V_max]))\n",
    "        np.save(os.path.join(c.data_path, \"optimal_natural_image/data\", f\"{model_name}_max5_{pn}.npy\"), np.array([total_U_max5, total_V_max5]))\n",
    "        np.save(os.path.join(c.data_path, \"optimal_natural_image/data\", f\"{model_name}_product_{pn}.npy\"), np.array(total_product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show the top 7 attention (product) images, and corresponding U map and V map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_names = [\n",
    "\"google/vit-base-patch16-224\",\n",
    "\"google/vit-base-patch32-384\",\n",
    "\"google/vit-large-patch16-224\",\n",
    "\"google/vit-large-patch32-384\",\n",
    "\"google/vit-huge-patch14-224-in21k\",\n",
    "\"facebook/dino-vitb16\",\n",
    "\"facebook/dino-vits16\",\n",
    "\"openai/clip-vit-base-patch16\",\n",
    "\"openai/clip-vit-base-patch32\",\n",
    "\"openai/clip-vit-large-patch14\",\n",
    "\"facebook/deit-base-distilled-patch16-224\",\n",
    "\"facebook/deit-small-distilled-patch16-224\",\n",
    "\"facebook/deit-tiny-distilled-patch16-224\"\n",
    "]\n",
    "\n",
    "im_list = os.listdir(c.imagenet_val_path)\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    (model, processor) = load_model(model_name) \n",
    "\n",
    "    if 'deit' in model_name: # deit has two extra tokens before image patch tokens.\n",
    "        start_index = 2\n",
    "    else:\n",
    "        start_index = 1\n",
    "\n",
    "    image_size = model.config.image_size\n",
    "    num_attention_heads = model.config.num_attention_heads\n",
    "    num_hidden_layers = model.config.num_hidden_layers\n",
    "    patch_size = model.config.patch_size\n",
    "    token_size = int(image_size / patch_size)\n",
    "\n",
    "    U_total = np.load(os.path.join(c.data_path, \"UVS\", f\"{model_name}_U_total.npy\"))\n",
    "    Vt_total = np.load(os.path.join(c.data_path, \"UVS\", f\"{model_name}_Vt_total.npy\"))\n",
    "\n",
    "    U_total = torch.from_numpy(U_total).float().to(c.device) # left singular vector\n",
    "    Vt_total = torch.from_numpy(Vt_total).float().to(c.device) # right singular vector\n",
    "\n",
    "    total_product_p = np.load(os.path.join(c.data_path, \"optimal_natural_image/data\", f\"{model_name}_product_p.npy\"))\n",
    "    total_product_n = np.load(os.path.join(c.data_path, \"optimal_natural_image/data\", f\"{model_name}_product_n.npy\"))\n",
    "\n",
    "    if not os.path.exists(os.path.join(c.data_path, f\"optimal_natural_image/figure/{model_name}_product_p\")):\n",
    "        os.makedirs(os.path.join(c.data_path, f\"optimal_natural_image/figure/{model_name}_product_p\"))\n",
    "        os.makedirs(os.path.join(c.data_path, f\"optimal_natural_image/figure/{model_name}_product_n\"))\n",
    "\n",
    "    for layer in range(num_hidden_layers):\n",
    "        for head in range(num_attention_heads):\n",
    "            print(f\"model: {model_name}, layer: {layer}, head: {head}\")\n",
    "            for sign,pn in [(1,'p'),(-1,'n')]:\n",
    "                fig, axs = plt.subplots(10, 22, figsize=(22, 10))\n",
    "                for mode in range(10):\n",
    "                    if sign == 1:\n",
    "                        order_product = np.argsort(total_product_p[:,layer,head,mode])\n",
    "                    else:\n",
    "                        order_product = np.argsort(total_product_n[:,layer,head,mode])\n",
    "                    for photo_i in range(7):\n",
    "                        im_no = order_product[-(photo_i+1)]\n",
    "                        im = PIL.Image.open(c.imagenet_val_path+im_list[im_no])\n",
    "                        im = im.resize((224,224))\n",
    "                        axs[mode,1+photo_i*3].imshow(im)\n",
    "                        axs[mode,1+photo_i*3].set_axis_off()\n",
    "                        # display cosine similarity and attention score.\n",
    "                        if sign == 1:\n",
    "                            attention_score = total_product_p[im_no,layer,head,mode]\n",
    "                        else:\n",
    "                            attention_score = total_product_n[im_no,layer,head,mode]\n",
    "                        axs[mode,1+photo_i*3].text(0, 20, f'{attention_score:.4g}', fontsize = 6)\n",
    "                        im = plt.imread(c.imagenet_val_path+im_list[im_no])\n",
    "                        if len(im.shape) < 3: # gray images\n",
    "                            im = np.repeat(im[:, :, np.newaxis], 3, axis=2)\n",
    "                        input = processor(images=im, return_tensors=\"pt\")\n",
    "                        input = input[\"pixel_values\"].float().to(c.device)\n",
    "                        output = model(input, output_hidden_states=True, output_attentions=False)\n",
    "                        hidden_states = output['hidden_states'][layer] # 1 batch, 14*14+1 token, 12 head * 64 embeding\n",
    "                        Umap = (hidden_states[:,:,:] @ (sign * U_total[layer,head,:,mode]))[0, start_index:].unflatten(0,(token_size,token_size)).detach().to('cpu').numpy()\n",
    "                        Vmap = (hidden_states[:,:,:] @ (sign * (Vt_total[layer,head,mode,:].T)))[0, start_index:].unflatten(0,(token_size,token_size)).detach().to('cpu').numpy()\n",
    "                        axs[mode,1+photo_i*3+1].imshow(Umap, cmap=\"gist_heat\", vmin=np.percentile(Umap,30), vmax=np.percentile(Umap,95))\n",
    "                        axs[mode,1+photo_i*3+2].imshow(Vmap, cmap=\"gist_heat\", vmin=np.percentile(Vmap,30), vmax=np.percentile(Vmap,95))\n",
    "                        axs[mode,1+photo_i*3+1].set_axis_off()\n",
    "                        axs[mode,1+photo_i*3+2].set_axis_off()\n",
    "\n",
    "                    cosine = torch.dot(U_total[layer,head,:,mode], Vt_total[layer,head,mode,:].T).detach().to('cpu').numpy()\n",
    "                    axs[mode,0].text(0.2,0.5,f\"{cosine:.3f}\")\n",
    "                    axs[mode,0].set_axis_off()\n",
    "\n",
    "                fig.tight_layout(pad=0.3)\n",
    "                plt.savefig(os.path.join(c.data_path, f\"optimal_natural_image/figure/{model_name}_product_{pn}/L{layer}h{head}_product_{pn}.png\"))\n",
    "                plt.clf()\n",
    "                plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
